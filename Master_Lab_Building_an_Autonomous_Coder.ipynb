{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Lab: Building an Autonomous Coding Assistant\n",
    "## From \"Script Kiddie\" to \"Senior Engineer\" with LangGraph\n",
    "\n",
    "**Objective:**\n",
    "In this comprehensive lab, we will build a single tool\u2014an **Autonomous Coding Assistant**\u2014that evolves as we learn. We will start with a simple script and progressively add intelligence using **LangGraph**.\n",
    "\n",
    "**Roadmap:**\n",
    "1.  **Level 1: The \"Script Kiddie\" (Linear Graph)**\n",
    "    *   *Concept*: State, Nodes, Edges.\n",
    "    *   *Goal*: A simple input-output chain.\n",
    "2.  **Level 2: The \"Trial & Error\" Coder (Tools & Cycles)**\n",
    "    *   *Concept*: Cycles, Conditional Edges, Tool Binding.\n",
    "    *   *Goal*: An agent that writes code and runs it.\n",
    "3.  **Level 3: The \"Code Reviewer\" (Reflection)**\n",
    "    *   *Concept*: Reflection, Multi-actor workflows.\n",
    "    *   *Goal*: An agent that critiques its own code before running.\n",
    "4.  **Level 4: The \"Senior Engineer\" (Reflexion)**\n",
    "    *   *Concept*: Reflexion (Reflection + External Knowledge), Complex State.\n",
    "    *   *Goal*: An agent that searches for solutions when stuck.\n",
    "\n",
    "---\n",
    "\n",
    "### Prerequisites\n",
    "Ensure you have the necessary libraries installed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langgraph langchain langchain_openai langchain_community tavily-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup & API Keys\n",
    "We will use `ChatOpenAI` (or `ChatWatsonx` if you prefer - just replace the initialization) and `Tavily` for search.\n",
    "\n",
    "**Note:** In this lab, we will use **simulated tools** for executing code (returning mock results) to keep things safe and simple in this notebook environment. However, the logic is identical to using a real `PythonREPL` tool.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# _set_env(\"OPENAI_API_KEY\") # Uncomment if running locally\n",
    "# _set_env(\"TAVILY_API_KEY\") # Uncomment if running locally\n",
    "\n",
    "# Setup Model (Replace with ChatWatsonx if needed, as per previous labs)\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0) \n",
    "# llm = ChatWatsonx(model_id=\"ibm/granite-3-2-8b-instruct\", ...) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 1: The \"Script Kiddie\" (Linear Graph)\n",
    "**Goal:** Build a simple linear graph that takes a user request and generates Python code.\n",
    "\n",
    "### The Concept: State\n",
    "In LangGraph, the **State** is the shared memory of your application. Nodes (functions) read from and write to this state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    request: str\n",
    "    code: str\n",
    "    \n",
    "# This defines the schema of our graph. \n",
    "# Nodes will receive 'State' and return a dict with updates (e.g. {\"code\": \"print('hi')\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Nodes\n",
    "Nodes are just Python functions. They take the current `State` and return a dictionary of updates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 1. Generation Node\n",
    "def generate_code_node(state: State):\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Write a Python script for the following request: {request}. Return ONLY the code, no markdown.\"\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    code = chain.invoke({\"request\": state[\"request\"]})\n",
    "    return {\"code\": code.content}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Graph\n",
    "We connect the nodes.\n",
    "`START` -> `generate_code_node` -> `END`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"coder\", generate_code_node)\n",
    "builder.add_edge(START, \"coder\")\n",
    "builder.add_edge(\"coder\", END)\n",
    "\n",
    "graph_l1 = builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Let's see what we built. We use Mermaid to visualize the graph structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph_l1.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Graph visualization requires 'pygraphviz' or similar. Error: {e}\")\n",
    "    # Fallback/Note: If this fails, it's usually missing binary dependencies. \n",
    "    # The structure is simply: START -> coder -> END\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Level 1\n",
    "Let's see it in action.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph_l1.invoke({\"request\": \"Print 'Hello LangGraph' 5 times\"})\n",
    "print(f\"Generated Code:\\n{result['code']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udfe2 Challenge: Switch Languages\n",
    "**Objective:** Modify the graph to generate **Java** code instead of Python.\n",
    "*Hint: You can modify the prompt in `generate_code_node`, or add a 'language' key to the State.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 2: The \"Trial & Error\" Coder (Tools & Cycles)\n",
    "**Goal:** Give the agent a tool to \"run\" the code. If it fails, the agent should loop back and fix it.\n",
    "\n",
    "### Simulated Tools\n",
    "We'll create a mock execution tool. Real execution is risky in notebooks without sandboxing, so we'll simulate it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def simulate_execution(code: str):\n",
    "    # Simulate various error conditions for testing\n",
    "    code_lower = code.lower()\n",
    "    \n",
    "    # Check for specific error patterns\n",
    "    if \"error\" in code_lower and \"print\" not in code_lower:\n",
    "        return \"Error: SyntaxError on line 1: unexpected token\"\n",
    "    \n",
    "    # Detect math.sqrt with negative number without cmath\n",
    "    if \"math.sqrt(-\" in code or (\"sqrt(-\" in code and \"cmath\" not in code):\n",
    "        return \"Error: ValueError: math domain error - cannot calculate square root of negative number\"\n",
    "    \n",
    "    return \"Execution Successful: Output: [1, 2, 3, 4, 5]\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated State\n",
    "We need to track more information now: the `execution_result` and the number of `iterations` (to prevent infinite loops).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "\n",
    "class ReActState(TypedDict):\n",
    "    request: str\n",
    "    code: str\n",
    "    execution_result: str\n",
    "    iterations: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_node(state: ReActState):\n",
    "    result = simulate_execution(state[\"code\"])\n",
    "    return {\n",
    "        \"execution_result\": result, \n",
    "        \"iterations\": state.get(\"iterations\", 0) + 1\n",
    "    }\n",
    "\n",
    "def fix_code_node(state: ReActState):\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Fix this code: {code}\\nError: {error}\\nReturn ONLY the fixed code.\"\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    new_code = chain.invoke({\"code\": state[\"code\"], \"error\": state[\"execution_result\"]})\n",
    "    return {\"code\": new_code.content}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Logic (The Loop)\n",
    "Nodes are not enough; we need **Conditional Edges** to decide where to go next.\n",
    "- If Error -> Go to `fixer`\n",
    "- If Success -> Go to `END`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: ReActState):\n",
    "    # Safe access: use .get() to avoid KeyError\n",
    "    execution_result = state.get(\"execution_result\", \"\")\n",
    "    iterations = state.get(\"iterations\", 0)\n",
    "    \n",
    "    if \"Error\" not in execution_result:\n",
    "        return \"end\"\n",
    "    if iterations > 3: # Safety valve\n",
    "        return \"end\"\n",
    "    return \"fix\"\n",
    "\n",
    "builder = StateGraph(ReActState)\n",
    "builder.add_node(\"coder\", generate_code_node) # We reuse the coder from Level 1\n",
    "builder.add_node(\"executor\", execute_node)\n",
    "builder.add_node(\"fixer\", fix_code_node)\n",
    "\n",
    "builder.add_edge(START, \"coder\")\n",
    "builder.add_edge(\"coder\", \"executor\")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"executor\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"fix\": \"fixer\" # If 'fix', go to 'fixer' node\n",
    "    }\n",
    ")\n",
    "builder.add_edge(\"fixer\", \"executor\") # After fixing, try executing again\n",
    "\n",
    "graph_l2 = builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(Image(graph_l2.get_graph().draw_mermaid_png()))\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Level 2 (Force a failure)\n",
    "We ask the LLM to write broken code (by asking for something invalid or using our trigger word 'error' implicitly) to see the loop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We modify the request to trick our mock executor\n",
    "result = graph_l2.invoke({\"request\": \"Write a python script that just says 'error' without printing it, to trigger a syntax error.\"})\n",
    "print(f\"Final Code: {result['code']}\")\n",
    "print(f\"Final Result: {result['execution_result']}\")\n",
    "print(f\"Iterations: {result['iterations']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udfe2 Challenge: Add a Tool\n",
    "**Objective:** Add a `read_file` tool node.\n",
    "1. Create a function `read_file_node` that returns `{\"file_content\": \"some config\"}`.\n",
    "2. Add it to the graph before the coder.\n",
    "3. Update the coder's prompt to use the file content.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 3: The \"Code Reviewer\" (Reflection)\n",
    "**Goal:** Code often has logical bugs that don't cause crashes. We need a \"Reviewer\" node that critiques the code *before* it runs.\n",
    "\n",
    "### The Reflection Pattern\n",
    "`Coder` -> `Reviewer` -> (Is Code Good?) -> `Yes` -> (Exec) / `No` -> `Reviser` -> `Reviewer`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectionState(TypedDict):\n",
    "    request: str\n",
    "    code: str\n",
    "    critique: str\n",
    "    revision_number: int\n",
    "\n",
    "def critique_node(state: ReflectionState):\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Review this code: {code}. If it looks correct and robust, output ONLY 'PERFECT'. Otherwise, list the brief issues.\"\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    critique = chain.invoke({\"code\": state[\"code\"]})\n",
    "    return {\"critique\": critique.content, \"revision_number\": state.get(\"revision_number\", 0) + 1}\n",
    "    \n",
    "def revise_node(state: ReflectionState):\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Original Code: {code}\\nCritique: {critique}\\nRewrite the code to fix the issues mentioned.\"\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    code = chain.invoke({\"code\": state[\"code\"], \"critique\": state[\"critique\"]})\n",
    "    return {\"code\": code.content}\n",
    "\n",
    "def should_revise(state: ReflectionState):\n",
    "    if \"PERFECT\" in state[\"critique\"] or state[\"revision_number\"] > 2:\n",
    "        return END\n",
    "    return \"revise\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(ReflectionState)\n",
    "builder.add_node(\"coder\", generate_code_node)\n",
    "builder.add_node(\"reviewer\", critique_node)\n",
    "builder.add_node(\"reviser\", revise_node)\n",
    "\n",
    "builder.add_edge(START, \"coder\")\n",
    "builder.add_edge(\"coder\", \"reviewer\")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"reviewer\",\n",
    "    should_revise,\n",
    "    {END: END, \"revise\": \"reviser\"}\n",
    ")\n",
    "builder.add_edge(\"reviser\", \"reviewer\") # Loop back for another review\n",
    "\n",
    "graph_l3 = builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization & Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(Image(graph_l3.get_graph().draw_mermaid_png()))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "result = graph_l3.invoke({\"request\": \"Write a python function to calculate the 100th fibonacci number efficiently.\"})\n",
    "print(f\"Critique: {result['critique']}\")\n",
    "print(f\"Final Code: {result['code']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udfe2 Challenge: Security Guard\n",
    "**Objective:** Change the Reviewer persona to be a \"Security Expert\".\n",
    "Make it reject any code that uses `eval()`, `exec()`, or `subprocess`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 4: The \"Senior Engineer\" (Reflexion)\n",
    "**Goal:** Combine everything. Code -> Exec -> Fail -> **Reflect on Error** -> **Search for Fix** -> Refine.\n",
    "\n",
    "This is the **Reflexion** pattern. It simulates a human engineer who hits a bug, reads the error, google it, and fixes the code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Reflexion State\n",
    "class ReflexionState(TypedDict):\n",
    "    request: str\n",
    "    code: str\n",
    "    execution_output: str\n",
    "    reflection: str\n",
    "    search_results: str\n",
    "    iterations: int\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tool for research\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import os\n",
    "\n",
    "# Safe Initialization: Use Real Tavily if key exists, else Mock it.\n",
    "try:\n",
    "    if \"TAVILY_API_KEY\" not in os.environ:\n",
    "        raise ValueError(\"No API Key found\")\n",
    "    tavily = TavilySearchResults(max_results=1)\n",
    "    print(\"\u2705 Tavily Search initialized (Real).\")\n",
    "except:\n",
    "    print(\"\u26a0\ufe0f TAVILY_API_KEY not found. Using Mock Search (Simulated).\")\n",
    "    class MockTavily:\n",
    "        def invoke(self, query):\n",
    "            # Return a structure similar to real Tavily\n",
    "            return [{\"url\": \"https://stackoverflow.com/simulated\", \"content\": \"The error 'name math is not defined' means you need to add 'import math' to your script.\"}]\n",
    "    tavily = MockTavily()\n",
    "\n",
    "def research_node(state: ReflexionState):\n",
    "    query = f\"How to fix {state['execution_output']}\"\n",
    "    # Now we can safely invoke 'tavily' (real or mock)\n",
    "    results = tavily.invoke(query)\n",
    "    return {\"search_results\": str(results)}\n",
    "\n",
    "def reflection_node(state: ReflexionState):\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Code: {code}\\nError: {error}\\nReason about why this failed. What exactly is wrong?\"\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    reflection = chain.invoke({\"code\": state[\"code\"], \"error\": state[\"execution_output\"]})\n",
    "    return {\"reflection\": reflection.content}\n",
    "    \n",
    "def refinement_node(state: ReflexionState):\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Fix code {code} based on error {error}, reflection {reflection} and search results {results}. Return ONLY code.\"\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    new_code = chain.invoke({\n",
    "        \"code\": state[\"code\"], \n",
    "        \"error\": state[\"execution_output\"],\n",
    "        \"reflection\": state[\"reflection\"],\n",
    "        \"results\": state[\"search_results\"]\n",
    "    })\n",
    "    return {\"code\": new_code.content, \"iterations\": state.get(\"iterations\", 0) + 1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 4 needs its own execute_node that uses 'execution_output' instead of 'execution_result'\n",
    "def execute_node_l4(state: ReflexionState):\n",
    "    result = simulate_execution(state[\"code\"])\n",
    "    return {\n",
    "        \"execution_output\": result,  # Note: execution_OUTPUT, not execution_result\n",
    "        \"iterations\": state.get(\"iterations\", 0) + 1\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the Master Graph\n",
    "builder = StateGraph(ReflexionState)\n",
    "\n",
    "builder.add_node(\"coder\", generate_code_node)\n",
    "builder.add_node(\"executor\", execute_node_l4)  # Use L4-specific executor\n",
    "builder.add_node(\"reflector\", reflection_node)\n",
    "builder.add_node(\"researcher\", research_node)\n",
    "builder.add_node(\"refiner\", refinement_node)\n",
    "\n",
    "builder.add_edge(START, \"coder\")\n",
    "builder.add_edge(\"coder\", \"executor\")\n",
    "\n",
    "def check_execution(state: ReflexionState):\n",
    "    # If success or too many retries, stop\n",
    "    # Safe access: use .get() to avoid KeyError\n",
    "    execution_output = state.get(\"execution_output\", \"\")\n",
    "    iterations = state.get(\"iterations\", 0)\n",
    "    \n",
    "    if \"Error\" not in execution_output or iterations > 2:\n",
    "        return END\n",
    "    return \"reflector\"\n",
    "\n",
    "builder.add_conditional_edges(\"executor\", check_execution, {END: END, \"reflector\": \"reflector\"})\n",
    "builder.add_edge(\"reflector\", \"researcher\")\n",
    "builder.add_edge(\"researcher\", \"refiner\")\n",
    "builder.add_edge(\"refiner\", \"executor\")\n",
    "\n",
    "graph_l4 = builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Master Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(Image(graph_l4.get_graph().draw_mermaid_png()))\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\ude80 The Final Test\n",
    "This test demonstrates the full Reflexion workflow. We ask for code that will initially fail, triggering the research and refinement cycle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This request will cause the LLM to generate code using math.sqrt(-1)\n",
    "# Our simulator will detect this and return an error\n",
    "# The agent will then reflect, search for a solution, and refine the code\n",
    "result = graph_l4.invoke({\"request\": \"Calculate the square root of -1 using the math module\"})\n",
    "print(\"\\n=== FINAL RESULT ===\")\n",
    "print(f\"Code:\\n{result['code']}\")\n",
    "print(f\"\\nExecution Output: {result.get('execution_output', 'N/A')}\")\n",
    "print(f\"Iterations: {result.get('iterations', 0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \u26a0\ufe0f Common Pitfalls & Debugging\n",
    "1.  **Infinite Loops**: Always have a `max_iterations` check in your conditional edges.\n",
    "    *   *Fix*: Add `if iterations > MAX: return END`.\n",
    "2.  **State Mismatch**: Ensure every node returns a dict that matches keys in your `TypedDict` State.\n",
    "    *   *Symptom*: Keys missing or TypeErrors.\n",
    "3.  **Graph RecursionLimit**: LangGraph has a default recursion limit (usually 25).\n",
    "    *   *Fix*: `graph.invoke(input, config={\"recursion_limit\": 50})`.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}